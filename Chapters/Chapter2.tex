%*******10********20********30********40********50********60********70********80

% For all chapters, use the newdefined chap{} instead of chapter{}
% This will make the text at the top-left of the page be the same as the chapter

%\chap{Background}
\chap{A review of past approaches to the analysis of bibliometric data and co-authorship networks}
\label{chapter2}
To create and analyze graphs representing co-authorship of research publications this dissertation will rely on two types of methods. The first are methods for recognition when two superficially different representations of an author's name correspond to the same author, which is called "author name disambiguation". The second set of methods relate to the representation of graphs and different measures used to quantify the importance of relationships among the components of the graph (including vertices and subgraphs). In the following sections we will overview both types of work.
\section{Author Name Disambiguation}
Author Name Disambiguation (AND) is required because multiple names can refer to the same author, many authors may share the same name due to abbreviations, name misspellings, or identical names in publications \cite{HanNamedisambiguationauthor2005}. %A well-known approach to solving this issue is termed as Author Name Disambiguation (AND). 
AND remains an important research focus in the computer science community, prompting to proposed solutions to control authorship with manual curation via participative individual and community effort such as the Author-ity project \cite{TorvikAuthornamedisambiguation2009}, DBLife \cite{DeRoseDBLifecommunityinformation2007}, the Open Researcher \& Contributor ID (ORCID) \cite{HaakORCIDsystemuniquely2012}, authorclaim.org, or researcherID.com. While most co-authorship analysis studies have tended to use a manual curation of AND \cite{SmalheiserAuthornamedisambiguation2009}, automatic approaches to AND involving supervised and unsupervised machine learning methods have also been proposed \cite{ferreira_brief_2012,giles_name_2005}. Unfortunately, the proposed solutions presented above are still in their infancy. They have several limitations in that they often target a unique bibliographic database and do not usually contain old or relatively recent records. The Author-ity database for example in its last release (as of June 2018), only includes PubMed and Medline AND records up to September 2008. Here, because our data span from 1996 to 2016, we leveraged on the work of Bilenko \cite{bilenko_learnable_2006} using an automatic, supervised fuzzy matching machine learning approach to disambiguate and normalize the bibliographic information collected (See section \ref{sec:meth_and}).

\section{Basic Descriptive Analysis for Network Graphs}
\label{sec:back_desc}
Vertex and edge characteristics are fundamental elements of network characterization. These characteristics are centered upon vertex and edge centrality measures. Although a vast number of different centrality measures have been proposed for the descriptive analysis of network graphs, the most common vertex and edge centrality measures are:
\begin{itemize}
\item Degree centrality: It is defined as the number of ties to a given author.
\item Betweenness centrality: it is the number of shortest paths between other pairs of vertices that go through a particular vertex. It relates to the perspective that importance relates to where a vertex is located with respect to the paths in the network graph. According to Freeman \cite{freeman_set_1977}, it is defined as:
\begin{equation} 
c_{B}(v)=\frac{\sigma (s,t|v)}{\sum_{s \neq t \neq v \in V}\sigma (s,t)} 
\end{equation} where $\sigma(s,t|v)$ is the total number of shortest paths between vertices $s$ and $t$ that pass through vertex $v$, and $\sigma (s,t)$ is the total number of shortest paths between $s$ and $t$ regardless of whether or not they pass through $v$.
\item Closeness centrality: the number of steps required for a particular author to access every other authors in the network. It captures the notion that a vertex is central if it is close to many other vertices. Considering a network $G=(V,E)$ where $V$ is the set of vertices and $E$, the set of edges, the closeness centrality $c_{Cl}(v)$ of a vertex $v$ is defined as:
\begin{equation} c_{Cl}(v)=\frac{1}{\sum_{u\in V}dist(v,u)} \end{equation} where $dist(v,u)$ is defined as the geodesic distance between the vertices $u,v \in V$.
\item Eigenvector centrality: degree to which an author is connected to other well connected authors in the network. It seeks to capture the idea that the more central the neighbors of a vertex are, the more central that vertex itself is. According to Bonacich \cite{bonacich_factoring_1972} and Katz \cite{katz_new_1953}, the Eigenvector centrality measure is defined as:
\begin{equation} c_{E_i}(v)=\alpha \sum_{\{u,v\}\in E}c_{E_i}(u) \end{equation} Where the vector $\mathbf{c}_{E_i}=(c_{E_i}(1),\dots ,c_{E_i}(N_v))^T$ is the solution to the eigenvalue problem $\mathbf{Ac}_{E_i}=\alpha^{-1}\mathbf{c}_{E_i}$, where $\mathbf{A}$ is the adjacency matrix for the network $G$. According to Bonacich \cite{bonacich_factoring_1972}, an optimal choice of $\alpha^{-1}$ is the largest eigenvalue of $\mathbf{A}$
\item Brokerage: degree to which a vertex occupy an advantageous position such that they can broker interactions between other vertices in the network.
\item Edge betweenness centrality extends from the notion of vertex centrality. It reflects the number of shortest paths traversing that edge. %This centrality measure was computed to assess which co-authorship collaboration ties are important for the flow of information.
\end{itemize}
\pagebreak
\subsection{Characterizing Network cohesion}
\label{sec:back_netChar}
%The extent to which subsets of authors are cohesive with respect to their relation in the co-authorship network was assessed through network cohesion. Specifically, we determined if collaborators (co-authors) of a given author tend to collaborate as well, and what subset of collaborating authors tend to be more productive in the network. 
There are many techniques to determine network cohesion \citep{kolaczyk_statistical_2014}:
\begin{itemize}
\item Cliques: According to Kolaczyk and Cs\'ardi \cite{kolaczyk_statistical_2014}, cliques are defined as complete subgraphs such that all vertices within the subset are connected by edges. %We computed the number of maximal cliques and assessed their size.
\item Density: Defined as the frequency of realized edges relative to potential edges, the density of a subgraph $H$ in $G$ provides a measure of how close $H$ is to be a clique in $G$. Density values vary between 0 and 1:
\begin{equation} 
den(H)=\frac{|E_H|}{|V_H|(V_H-1)/2} 
\end{equation}

\item Transitivity: The transitivity of $G$ is a measure of the relative frequency of $G$ defined as: 
\begin{equation} 
cl_T = \frac{3\tau_\Delta (G)}{\tau_3 (G)} 
\end{equation}
where $\tau_\Delta (G)$ is the number of triangles in $G$, and $\tau_3 (G)$ is the number of connected triples (sometimes referred to as 2-star). This measure is also referred to as the fraction of transitive triples. It represents a measure of global clustering of $G$ summarizing the relative frequency with which connected triples close to form triangles \cite{kolaczyk_statistical_2014}.
\item Connectivity, Cuts, and Flows: The concepts of vertex and edge cuts is derived from the concept of vertex (edge) connectivity. The vertex (edge) connectivity of a graph $G$ is the largest integer such that $G$ is k-vertex- (edge-) connected \cite{kolaczyk_statistical_2014}. These measures helped assess the most important vertices for information flow and the long-term sustainability of each network. Since co-authorship networks are undirected graphs, the concept of weak and strong connectivity is irrelevant. A graph $G$ is said to be connected if every vertex in $G$ is reachable from every other vertex. Usually, one of the connected components can dominate the others, hence the concept of giant component. The giant component characterizes the connectedness of the vertices in the network. 
\item Graph Partitioning: Regularly framed as a community detection problem, %we applied graph partitioning to find subsets of vertices that demonstrate a 'cohesiveness' with respect to their underlying relational patterns. 
graph partitioning identifies cohesive subsets of vertices generally well connected among themselves and well separated from the other vertices in the network graph. Two established methods of graph partitioning are Hierarchical clustering (agglomerative vs divisive) and Spectral clustering \cite{kolaczyk_statistical_2014}. %In our research, we applied agglomerative Hierarchical Clustering to the co-authorship networks.
\end{itemize}

\section{Modeling of Network data}
\label{sec:back_model}
The purposes of network graph modeling are to test significance of the characteristics of observed network graphs, and to study proposed mechanisms of real-world networks such as degree distributions and small-world effects \cite{kolaczyk_statistical_2014}. A model for a network graph is a collection of possible graphs $\mathscr{G}$ with a probability distribution $\mathbb{P}_\theta$ defined as:
\begin{equation} 
\{ \mathbb{P}_\theta\ (G), G \in \mathscr{G} : \theta \in \Theta \} 
\end{equation}
where $\theta$ is a vector of parameters ranging over values in $\Theta$.\\
Given an observed network graph $G^{obs}$ and some structural characteristics $\eta (\cdot)$, our goal is to assess if $\eta (G^{obs})$ is unusual. We then compare $\eta (G^{obs})$ to collection of values $\{\eta(G):G \in \mathscr{G}\}$. If $\eta (G^{obs})$ is too extreme with respect to this collection, then we have enough evidence to assert that $\eta (G^{obs})$ is not a uniform draw from $\mathscr{G}$. %\\

%\pagebreak
\subsection{Mathematical models for Network Graphs}
\label{sec:back_mathModel}
There are mainly four proposed mathematical models for network graphs:
\begin{itemize}
\item Classical Random Graph Models: First established by Erd\H os and R\'enyi \cite{erdos_random_1959,erdos_evolution_1960,erdos_strength_1964}, it specifies a collection of graphs $\mathscr{G}$ with a uniform probability $\mathbb{P}(\cdot)$ over $\mathscr{G}$. A variant of this model called the Bernoulli Random Graph Model was also defined by Gilbert \cite{gilbert_random_1959}.
\item Generalized Random Graph Models: These models emanated from the generalization of Erd\H os and R\'enyi's formulation, defining a collection of graphs $\mathscr{G}$ with prespecified degree sequence.
\item Mechanistic Network Graph Models: These models mimic real-world phenomena and include Small-World Models commonly referred to as "six-degree separation". It was introduced by Watts and Strogatz \cite{watts_collective_1998} and have since received a lot of interests in the existing literature especially in Neuroscience. Small-world networks usually exhibit high levels of clustering and small distances between vertices. Classical models are not fit to better represent such behaviors since they usually display low levels of clustering and small distance between vertices. Examples of known small-world networks include the network of connected proteins  or the transcriptional networks of genes \cite{van_noort_yeast_2004}. A variant of Small-World models is the Preferential Attachment Models defined based on the popular principle of "the rich get richer". Preferential attachment models gained fascination after the work of Barab\'asi and Albert who studied the growth of the World Wide Web \cite{barabasi_emergence_1999}. Examples of Preferential Attachment networks include that of the World Wide Web and the scientific citation network \cite{albert_internet:_1999,jeong_measuring_2003}. An important characteristic of these models is that as time tend to infinity, there degree distribution tends to follow a power law.
\end{itemize}

\subsection{Statistical models for Network Graphs}
\label{sec:back_statModel}
Although mathematical models tend to be simpler than statistical models, the latter allow model fitting and assessment. Various classes of network graph models have been proposed. Here, we present the three main classes of statistical network models and a version of ERGM adapted to temporal snapshots:

\subsubsection{Stochastic Block Model}
\label{sec:back_sbm}
Blockmodel is a statistical method to identify, in a given network, clusters or classes of authors that share structural characteristics \cite{lorrain_structural_1971,doreian_generalized_2004}. Each such cluster forms a position. The units within a cluster have the same or similar connection patterns. Given a graph $G=(V,E)$ and its adjacency matrix $\mathbf{Y}$, for two distinct nodes $i,j \in V$, the block model defined by Kolaczyk and Cs\'ardi \cite{kolaczyk_statistical_2014}, specifies that each element $Y_{ij}$ of $\mathbf{Y}$ is conditional on the class label $q$ and $r$ of the vertices $i$ and $j$. The model has the form:
\begin{equation}Pr(\mathbf{Y}=\mathbf{y})=\left( \frac{1}{\kappa} \right) exp \set*{ \sum_{q,r} \theta_{qr} L_{qr}(\mathbf{y}) }\end{equation}
where $L_{qr}$ is the number of edges in the observed graph $\mathbf{y}$ connecting vertices of classes $q$ and $r$, $\theta_{qr}$ is the parameter estimates, and $\kappa$ is a normalization constant defined as:
\begin{equation}
\kappa = \sum_{\mathbf{y}}exp \set*{ \sum_{q,r} \theta_{qr} L_{qr}(\mathbf{y}) }
\end{equation}
Stochastic block model (SBM) originated from the ideas that equivalent units can be grouped together. There are three definitions of equivalences which are structural, automorphic and regular \cite{mali_dynamic_2012}. In practice, the differences in types of equivalence tend to blur when stochastic block modeling is applied to real networks.

\subsubsection{Exponential Random Graph Model}
\label{sec:back_ergm}
Also referred to as p* models, Exponential Random Graph Models (ERGMs) are probability models for network designed in analogy to Generalized Linear Models (GLMs) \cite{kolaczyk_statistical_2014}. ERGMs have gained increasing interests especially in modeling social networks. Robins et al. \cite{robins_introduction_2007} provide a nice introduction to ERGM as well as a general framework for ERGM creation which we closely followed in this dissertation. \\
Given a random graph $G=(V,E)$, for two distinct nodes $i,j \in V$, we define a random binary variable $Y_{ij}$ such that $Y_{ij}=1$ if there is an edge $e \in E$ between $i$ and $j$, and $Y_{ij}=0$ otherwise. Since co-authorship networks are by definition undirected networks, $Y_{ij}=Y_{ji}$ and the matrix $\mathbf{Y}=\left[Y_{ij}\right]$ represents the random adjacency matrix for $G$. The general formulation of ERGM is therefore:
\begin{equation}
Pr(\mathbf{Y}=\mathbf{y})=\left( \frac{1}{\kappa} \right) exp \set*{ \sum_{H} \theta_H g_H(\mathbf{y})}
\end{equation}
where each $H$ is a configuration, a set of possible edges among a subset of the vertices in $G$ and $g_H(\mathbf{y})=\prod_{y_{ij} \in H}y_{ij}$ is the network statistic corresponding to the configuration $H$; $g_H(\mathbf{y})=1$ if the configuration is observed in the network $\mathbf{y}$, and is $0$ otherwise. $\theta_H$ is the parameter corresponding to the configuration $H$ (and is non-zero only if all pairs of variables in $H$ are assumed to be conditionally dependent); $\kappa$ is a normalization constant defined as:
\begin{equation}
\kappa = \sum_{\mathbf{y}}exp \set*{ \sum_{H} \theta_H g_H(\mathbf{y})}
\end{equation}

\subsubsection{Temporal Exponential Random Graph Model}
\label{sec:back_tergm}
The Temporal Exponential Random Graph Model (TERGM) is an extension of the ERGM described in section \ref{sec:methods_ergm} proposed by Hanneke, Fu, and Xing \cite{hanneke_discrete_2010} from the work of Robins and Pattison \cite{robins_random_2001}. The TERGM was designed with the idea of accounting for inter-temporal dependence in longitudinally collected network data. For a full description of the TERGM, we refer the reader to Leifeld, Cranmer, and Desmarais \cite{leifeld_temporal_2015}. 

\pagebreak
\subsubsection{Latent Network Model}
\label{sec:back_lnm}
Designed in analogy to Mixed Models, Latent Network Models (LNM) allow the incorporation of latent or unobserved variables in network modeling. These models specifically account for structural equivalence, to model hidden factors or information not available in the network. Kolaczyk and Cs\'ardi \cite{kolaczyk_statistical_2014} provide a formulation of LNM. Given the adjacency matrix $\mathbf{Y}$ of a graph $G=(V,E)$, for each element $Y_{ij}$ of $\mathbf{Y}$, the latent variable model is of the form:
\begin{equation}Y_{ij}=h(\theta,z_i,z_j,\epsilon_{ij})\end{equation} where $\theta$ is a constant, the $\epsilon_{ij}$ are independent and identically distributed pair-specific effects, and $h$ is a symmetric function. The model assumes that each vertex $i\in V$ has a latent variable $z_i$. Considering observed covariates $\mathbf{Z}$, the probability of forming an edge between two nodes $i$ and $j$ ($i,j \in V$) is independent of all other vertex pairs given values of latent variables, and is defined as:
\begin{equation}
Pr(\mathbf{Y}|\mathbf{Z},\theta)=\prod_{i\neq j}Pr\left(Y_{ij}|z_i,z_j,\theta \right)
\end{equation}
%Hoff \cite{hoff_modeling_2008} suggested an approach based upon the principles of eigen-analysis of specifying latent variables which we followed in this dissertation.
